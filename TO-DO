# Baseline GTA-clean+MS+VGG
10/05
(HP) 1. Multi-feature agnostic
  - 回歸ＤＳＮ設計，確定做在output space或feature space哪個performance比較好。
(HP) 2. When do we start using s2t translated labels ?
  - 理論上一開始的s2t quality比較低或比較模糊，可能導致產生的圖片與source labels不相符，測試在後面一點開始transfer是否能增進performance與穩定度。
(WC) 3. Disable s2t translated label (i.e., only keep DSN architectures and comapred to Dennis')
  - Ablatino study. 如果這個設定會比enable差表示translated label有用，除此之外如果該performance比Dennis好，我們同時可以證明ＤＳＮ架構有好處。
(WC) 4. Add a regularization to minimize the l1 norm of F_s and F_s2t
  - translated labels某種程度而言也是在要求轉換過後的圖應與source經過shared encoder後的output應一致。該實驗探討直接在feature space上做是否有更多好處。
(HP) 5. Check if private encoders work
  - one of our main contribution is the DSN architecture, so it's important to check if our private encoder works as a part of DSN.
(WC) 6. Modify the decoder archicture into "Y" shape
  - improve the s2t image quality, because we think s2t is sharper than target image
(HP) 7. Dennis code run on GTA_Clean

10/12
(WC) 8. Pretrain private encoder and decoder -> freeze shared encoder layer3, layer4
  - We think the different initial method for the model will cause imbanlance problem and avoid private encoder working. 
(HP) 9. Add style transfer skill "Adain" on the image translation loss. Low level feature of VGG, and match channel wise mean
  - Walon suggest this idea make sense for style transfer
(HP) 10. Which layer of feature is better to be the input of the private encoder ? layer0, layer1
  - make sure which layer is better for disentanglement
(HP) 11. latent reconstruction
  - Walon think it will be benefitial for disentanglement


10/15
Three possible ways
  - Verify segmentation does not contain texture information in deeper levels
  - Keep DSN! Tuning todo_9 to disentangle texture and structure successfully and achieve similar performance meanwhile
  - Pure augmentation
  
(WC) 12. Traing hyper-base without tranlsated labels
(WC) 13. Verify our assumption. Use only layer4 feats to reconstrcut original input images.
  - Fixed Dennis's segmentation model weights to do so
  - With only one decoder and no other inputs.
(HP) 14. Tuning todo_9
  a. Training todo_9 without adversarial loss
    - check if it can generate better image quality and keep the structure at the same time
  b. Adjusting weights of mean matching [1/8, 1/16, 1/32, 0 ,0]
    - just adjust
  c. Training todo_9 with variance matching only
    - just train
  d. Training todo_9 with variance and mean matching
    - just train la !
  e. Training todo_9 without translated labels (lower priority)
    - todo_9 is not stable yet. It's meaningless to do this.
(WC) 15. Using the same encoder for both s2t and t2s images
  - Ideally, source dataset contains variety of styles which helps the encoder to learn different styles.
  In this experiement, we hope that by using the same encoder, target images can benefit from various source styles and generate more styles.
  - Remove the domain codes
  
10/19
  Experiments may contain:
    - GTA5 -> CityScapes segmentation performance
    - SYNTHIA -> CityScapes segmentation performance
    - Ablation study for loss choice
    - Ablation study for differnet layer choice
    - Visualization of prediction segmentation map
    - Visualization of disentangle ability
    
  16. Full-image hyper base
  17. Hyper base with reconstruction weight self-mean, self-var merge
  18. SYNTHIA loader
  
  ultimate-base crop
  (WC) 19. ultimate-base, private encoder input = layer3
  (WC) 20. source only
  (WC) 21. DSN w/o adversarial loss
  (WC) 22. Dennis model
  (WC) 23. True DeepLab2 shared-encoder
  (WC) 24. PSP shared-encoder 
  (HP) 25. DSN w/ 2x segmentation loss
  (WC) 26. generate s2t image and train another segmentation model

10/28
  (HP) 27. tune DSN
    a. tran rec weights [1/2, 1/4, 1/8, 1, 1]
    b. DSN w/ rec loss only
    c. DSN w/ layer 3 seg prediction
  (WC) 28. ultimate base with aspp discriminator
  (WC) 29. tune psp
# CVPR template 
(HP) do it 

 10/29
 (HP) 30. Generate confusion matrix of DSN 
  - Although DSN can learn domain-invariant features, it might suffer from mis-alignment problem. confusion matrix
 (HP) 31. considering other contribution, for example, how can we utilize t2s.
    a. translation starts from 0 step
    b. translation starts from 15000 step
 b. if we only bias to source-dataset, there might be domain gaps. If we bias to translated images, it might suffer from error propogation.
 
 11/1
  (WC) 32. Semi-supervised with DSN
  
 11/2
  (HP) 33. source only + DSN (single-level)
  (HP) 34. Check source only confusion matrix
  (WC) 35. Pretrain private encoders and decoders with Dennis' weights
  
 11/6
       36. ultimate_base_single mean weights [1, 0.5, 0.25]
        a. [0.5, 0.5, 0.5]
       37. todo33 w/o trans rec
        - to show the difference between ours and dsn

 11/20 -- supplement
  VIDEO
       idea 同一個cityscapes的video sequence左邊是原圖右邊是轉換過的圖片，然後做成像video一樣可以播放
  (HP) v1. seg_map 原圖, source only, seg-adaptation, ours w/ LT 四種video比較
  (WC) v2. translation video 證明學到structure GTA TO CITYSCPAE, CITYSCPAE TO GTA 
  (WC) v3. restoration video 證明學到structure GTA TO CITYSCPAE TO GTA, CITYSCPAE TO GTA TO CITYSCPAE
  (HP) v4. 玩ＧＴＡ囉 收集影片 去崔佛家 -- DONE
        先找一段cityscapes video seq.
        產seg map video: source only, dennis, ultimate_base
        研究一下怎麼剪影片
  Paper
       GTAV
  (WC) p1. rectore     imege -> City to GTAV to City, Synthia to GTAV to Synthia 各三組
       SYNTHIA
  (HP) p2. seg_map -> source only, seg-adpt, DISE 五組
  (WC) p3. translation image -> City to Synthia , Synthia to City 各三組
  (WC) p4. rectore     imege -> City to Synthia to City, Synthia to City to Synthia 各三組
      
